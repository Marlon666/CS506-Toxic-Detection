{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./gunControlComments.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>As a reminder, this subreddit [is for civil di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fiending4Krokodil</td>\n",
       "      <td>&gt;\"Not only do traditional notions of masculini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spiel_Foss</td>\n",
       "      <td>In a country where people with insurance can't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DaGoz</td>\n",
       "      <td>Another way is to prevent their access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joyRN</td>\n",
       "      <td>Why not both?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Username                                            Comment\n",
       "0      AutoModerator  As a reminder, this subreddit [is for civil di...\n",
       "1  Fiending4Krokodil  >\"Not only do traditional notions of masculini...\n",
       "2         Spiel_Foss  In a country where people with insurance can't...\n",
       "3              DaGoz  Another way is to prevent their access to the ...\n",
       "4              joyRN                                      Why not both?"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394.0732213246768, 590.7202819048923, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = train.comment_text.str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate           none  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 153164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('./train.csv').fillna(' ')\n",
    "test = pd.read_csv('./altRightSubredditBannedComments.csv').fillna(' ')\n",
    "train = train.iloc[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['Comment']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '00',\n",
       " '000',\n",
       " '000000',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '084080',\n",
       " '09',\n",
       " '0px',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '1034',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '16th',\n",
       " '17',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '183',\n",
       " '184',\n",
       " '185',\n",
       " '186',\n",
       " '1867',\n",
       " '187',\n",
       " '1872',\n",
       " '188',\n",
       " '1884',\n",
       " '189',\n",
       " '1895',\n",
       " '1896',\n",
       " '18th',\n",
       " '19',\n",
       " '190',\n",
       " '1901',\n",
       " '1905',\n",
       " '191',\n",
       " '1911',\n",
       " '1912',\n",
       " '1914',\n",
       " '1915',\n",
       " '1917',\n",
       " '1918',\n",
       " '192',\n",
       " '1920',\n",
       " '1923',\n",
       " '193',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1931',\n",
       " '1936',\n",
       " '1939',\n",
       " '194',\n",
       " '1940',\n",
       " '1942',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '195',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1956',\n",
       " '1958',\n",
       " '196',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1962',\n",
       " '1964',\n",
       " '1965',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1974',\n",
       " '1975',\n",
       " '1978',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '199',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1b',\n",
       " '1ezju1ks9nu',\n",
       " '1px',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '20th',\n",
       " '21',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '22nd',\n",
       " '23',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '244',\n",
       " '245',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '254',\n",
       " '255',\n",
       " '26',\n",
       " '26th',\n",
       " '27',\n",
       " '27_noticeboard',\n",
       " '28',\n",
       " '28th',\n",
       " '29',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30th',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '350',\n",
       " '36',\n",
       " '360',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '3a',\n",
       " '3o',\n",
       " '3px',\n",
       " '3rd',\n",
       " '3rr',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '40s',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '466',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '4meter4',\n",
       " '4th',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '585',\n",
       " '59',\n",
       " '5em',\n",
       " '5p',\n",
       " '5ri7ka',\n",
       " '5th',\n",
       " '6',\n",
       " '60',\n",
       " '60mins',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '6th',\n",
       " '7',\n",
       " '70',\n",
       " '706',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '7e',\n",
       " '7th',\n",
       " '8',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '8px',\n",
       " '8th',\n",
       " '9',\n",
       " '90',\n",
       " '91',\n",
       " '910',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '966',\n",
       " '97',\n",
       " '978',\n",
       " '98',\n",
       " '99',\n",
       " '998',\n",
       " '99th',\n",
       " '9th',\n",
       " '_',\n",
       " 'a1',\n",
       " 'a2',\n",
       " 'a4',\n",
       " 'a7',\n",
       " 'aa',\n",
       " 'aaron',\n",
       " 'abandoned',\n",
       " 'abbey',\n",
       " 'abc',\n",
       " 'abdul',\n",
       " 'abide',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abolished',\n",
       " 'aboriginal',\n",
       " 'aboriginals',\n",
       " 'abortion',\n",
       " 'abraham',\n",
       " 'abramoff',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abu',\n",
       " 'abul',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academically',\n",
       " 'academics',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'acceleration',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'acclaim',\n",
       " 'accommodate',\n",
       " 'accompanied',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'acids',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledging',\n",
       " 'acorn',\n",
       " 'acousmatic',\n",
       " 'acquainted',\n",
       " 'acquired',\n",
       " 'acres',\n",
       " 'acronym',\n",
       " 'acronyms',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acupuncture',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'ade',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adhered',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjustments',\n",
       " 'admi',\n",
       " 'admin',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'administrators',\n",
       " 'admins',\n",
       " 'adminship',\n",
       " 'admire',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'adolf',\n",
       " 'adolph',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'advertisement',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advisory',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'advocates',\n",
       " 'advocating',\n",
       " 'ae',\n",
       " 'aeropagitica',\n",
       " 'aesthetic',\n",
       " 'afc',\n",
       " 'afd',\n",
       " 'afds',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affiliated',\n",
       " 'affiliation',\n",
       " 'affirm',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afghanistan',\n",
       " 'aforementioned',\n",
       " 'aforesaid',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africans',\n",
       " 'afterlife',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agendas',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agf',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressor',\n",
       " 'aggressors',\n",
       " 'agile',\n",
       " 'agk',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'agricultural',\n",
       " 'agw',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahmed',\n",
       " 'ai',\n",
       " 'ai009',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'ainsworth',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airport',\n",
       " 'airways',\n",
       " 'aiv',\n",
       " 'aka',\n",
       " 'akin',\n",
       " 'akkadian',\n",
       " 'al',\n",
       " 'alain',\n",
       " 'alan',\n",
       " 'alaska',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albanians',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberto',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'alcoholism',\n",
       " 'alene',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alfred',\n",
       " 'algebra',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'ali',\n",
       " 'alien',\n",
       " 'align',\n",
       " 'aligned',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'allah',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegory',\n",
       " 'allen',\n",
       " 'alliance',\n",
       " 'allies',\n",
       " 'allmusic',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'almighty',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'alr',\n",
       " 'alright',\n",
       " 'alt',\n",
       " 'altaic',\n",
       " 'alter',\n",
       " 'altered',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'altogether',\n",
       " 'altright',\n",
       " 'alumni',\n",
       " 'alvarez',\n",
       " 'alyssa',\n",
       " 'ama',\n",
       " 'amanda',\n",
       " 'amateur',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'amazons',\n",
       " 'ambassador',\n",
       " 'ambedkar',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambitious',\n",
       " 'amcom',\n",
       " 'amd',\n",
       " 'amend',\n",
       " 'amended',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amir',\n",
       " 'amounts',\n",
       " 'ample',\n",
       " 'amusement',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'anal',\n",
       " 'analogy',\n",
       " 'analyse',\n",
       " 'analysis',\n",
       " 'analyze',\n",
       " 'anarchism',\n",
       " 'anarchist',\n",
       " 'anarcho',\n",
       " 'ancestor',\n",
       " 'ancestors',\n",
       " 'ancestry',\n",
       " 'ancient',\n",
       " 'andemu',\n",
       " 'anderson',\n",
       " 'andhra',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andreas',\n",
       " 'andrew',\n",
       " 'andy',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'anger',\n",
       " 'angika',\n",
       " 'angle',\n",
       " 'anglo',\n",
       " 'angry',\n",
       " 'ani',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annexation',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'anomaly',\n",
       " 'anon',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'antagonistic',\n",
       " 'antarctica',\n",
       " 'ante',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anthropological',\n",
       " 'anti',\n",
       " 'antisemite',\n",
       " 'antisemitism',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'aol',\n",
       " 'apart',\n",
       " 'apartheid',\n",
       " 'apocalyptic',\n",
       " 'apollo',\n",
       " 'apollonian',\n",
       " 'apologies',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apology',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'applicable',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approves',\n",
       " 'approving',\n",
       " 'approximately',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'arabic',\n",
       " 'arabs',\n",
       " 'aramaic',\n",
       " 'aran',\n",
       " 'arass',\n",
       " 'arbcom',\n",
       " 'arbcomm',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arbitration',\n",
       " 'arbitrator',\n",
       " 'arbitrators',\n",
       " 'arc',\n",
       " 'arch',\n",
       " 'archaeological',\n",
       " 'archaeology',\n",
       " 'archie',\n",
       " 'architecture',\n",
       " 'archive',\n",
       " 'archive1',\n",
       " 'archived',\n",
       " 'archives',\n",
       " 'archiving',\n",
       " 'arctic',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arena',\n",
       " 'arent',\n",
       " 'arf',\n",
       " 'argentina',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'arguement',\n",
       " 'argues',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'ariel',\n",
       " 'aristotle',\n",
       " 'arithmetic',\n",
       " 'arjun',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'armagh',\n",
       " 'armed',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'armies',\n",
       " 'arms',\n",
       " 'armstrong',\n",
       " 'army',\n",
       " 'aromanian',\n",
       " 'arose',\n",
       " 'arrange',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'arse',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articles_for_deletion',\n",
       " 'artie',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'artwork',\n",
       " 'arvada',\n",
       " 'arvanitic',\n",
       " 'aryan',\n",
       " 'aryans',\n",
       " 'arzel',\n",
       " 'asad',\n",
       " 'asap',\n",
       " 'ascencio',\n",
       " 'ashamed',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'asiana',\n",
       " 'asians',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'asp',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'aspx',\n",
       " 'ass',\n",
       " 'assad',\n",
       " 'assassination',\n",
       " 'assault',\n",
       " 'assert',\n",
       " 'asserted',\n",
       " 'asserting',\n",
       " 'assertion',\n",
       " 'assertions',\n",
       " 'asserts',\n",
       " 'assess',\n",
       " 'assessed',\n",
       " 'assessing',\n",
       " 'assessment',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'assigned',\n",
       " 'assignment',\n",
       " 'assignments',\n",
       " 'assigns',\n",
       " 'assimilated',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'associates',\n",
       " 'association',\n",
       " 'associations',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assumes',\n",
       " 'assuming',\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9415531146623922\n",
      "CV score for class severe_toxic is 0.9784671555524577\n",
      "CV score for class obscene is 0.9684617541108383\n",
      "CV score for class threat is 0.982687003455993\n",
      "CV score for class insult is 0.9572395446078321\n",
      "CV score for class identity_hate is 0.9477997251985407\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['Username']})\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CV score is 0.9627013829313423\n"
     ]
    }
   ],
   "source": [
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we are going to try LSTM here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"Comment\"].fillna(\"CVxTz\").values\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "9000/9000 [==============================] - 96s 11ms/step - loss: 0.1840 - acc: 0.9548 - val_loss: 0.1054 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10542, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "9000/9000 [==============================] - 102s 11ms/step - loss: 0.0812 - acc: 0.9726 - val_loss: 0.0752 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10542 to 0.07521, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./submission.csv\")\n",
    "\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "\n",
    "\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-0d0c84874901>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-0d0c84874901>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
