{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin with analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "train = train.iloc[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n, editors don\\'t care about your \"\"explanations\"\" if they\\'re not accompanied by reliable published sources. I could \"\"explain\"\" why I thought dogs ate cats but I would get the same reception if I didn\\'t provide sources. Stop edit warring and present sources.  talk to me \"'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'][192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will first try to use Naive Bayes to analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that our train data is the comments with several labels, which is from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the model with a bag of word and ngram as mentioned in the paper\n",
    "re_tok = re.compile('([' + string.punctuation + '“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "train['none'] = 1-train[label_cols].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_Bayes(test_path, result):\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    n = train.shape[0]\n",
    "    test = pd.read_csv(test_path).fillna(' ')\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1 )\n",
    "    trn_term_doc = vec.fit_transform(train['comment_text'])\n",
    "    test_term_doc = vec.transform(test['Comment'])\n",
    "    x = trn_term_doc\n",
    "    test_x = test_term_doc\n",
    "    preds = np.zeros((len(test), len(label_cols)))\n",
    "\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fit', j)\n",
    "        m,r = get_mdl(train[j], x)\n",
    "        preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "        \n",
    "    subm = pd.read_csv('./submission.csv')\n",
    "    submid = pd.DataFrame({'id': test[\"Comment ID\"]})\n",
    "    subcommet = pd.DataFrame({'comment': test[\"Comment\"]})\n",
    "    submission = pd.concat([submid, subcommet, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "    submission.to_csv(result, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here's the basic naive bayes feature equation:\n",
    "def pr(y_i, y, x):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "#Fit a model for one dependent at a time:\n",
    "def get_mdl(y, x):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y, x) / pr(0,y, x))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes('./input/politic/altRightSubredditBannedComments.csv', 'altRightNB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes('./input/politic/abortionComments.csv', 'abortionCommentsNB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes('./input/politic/donaldTrumpElectionWinComments.csv', 'DTLRNB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes('./input/politic/gunControlComments.csv', 'gunCNB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then we can try logic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "#linear regression model training\n",
    "def linear_regression_train_model(test_path, result):\n",
    "    train = pd.read_csv('./train.csv').fillna(' ')\n",
    "    test = pd.read_csv(test_path).fillna(' ')\n",
    "    train = train.iloc[0:50000]\n",
    "    train_text = train['comment_text']\n",
    "    test_text = test['Comment']\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=10000)\n",
    "    word_vectorizer.fit(all_text)\n",
    "    train_word_features = word_vectorizer.transform(train_text)\n",
    "    test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words='english',\n",
    "        ngram_range=(2, 6),\n",
    "        max_features=50000)\n",
    "    char_vectorizer.fit(all_text)\n",
    "    train_char_features = char_vectorizer.transform(train_text)\n",
    "    test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "    scores = []\n",
    "    submission = pd.DataFrame.from_dict({'id': test['Username']})\n",
    "    for class_name in class_names:\n",
    "        train_target = train[class_name]\n",
    "        classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "\n",
    "        cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "        scores.append(cv_score)\n",
    "        print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "        classifier.fit(train_features, train_target)\n",
    "        submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "    print('Total CV score is {}'.format(np.mean(scores)))\n",
    "    \n",
    "    subcommet = pd.DataFrame({'comment': test[\"Comment\"]})\n",
    "    submission = pd.concat([subcommet, submission], axis=1)\n",
    "\n",
    "    submission.to_csv(result, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9596636113784185\n",
      "CV score for class severe_toxic is 0.9859851084008343\n",
      "CV score for class obscene is 0.977594990154138\n",
      "CV score for class threat is 0.9832288967021358\n",
      "CV score for class insult is 0.971418750360742\n",
      "CV score for class identity_hate is 0.9650141645863023\n",
      "Total CV score is 0.9738175869304285\n"
     ]
    }
   ],
   "source": [
    "linear_regression_train_model('./input/altRightSubredditBannedComments.csv', 'altRightLR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9597384235909963\n",
      "CV score for class severe_toxic is 0.9859677900792415\n",
      "CV score for class obscene is 0.9776991351208171\n",
      "CV score for class threat is 0.9831342936375068\n",
      "CV score for class insult is 0.9714706555451095\n",
      "CV score for class identity_hate is 0.9652301024325013\n",
      "Total CV score is 0.9738734000676955\n"
     ]
    }
   ],
   "source": [
    "linear_regression_train_model('./input/abortionComments.csv', 'abbortionCommentsLR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9588854163155663\n",
      "CV score for class severe_toxic is 0.9860032958677424\n",
      "CV score for class obscene is 0.9767734828122561\n",
      "CV score for class threat is 0.9836402748118425\n",
      "CV score for class insult is 0.9709402822376224\n",
      "CV score for class identity_hate is 0.9647623193377864\n",
      "Total CV score is 0.9735008452304695\n"
     ]
    }
   ],
   "source": [
    "linear_regression_train_model('./input/politic/donaldTrumpElectionWinComments.csv', 'DTLR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.959762972885201\n",
      "CV score for class severe_toxic is 0.985969822436429\n",
      "CV score for class obscene is 0.9777064440396698\n",
      "CV score for class threat is 0.9831557940417873\n",
      "CV score for class insult is 0.97148863653939\n",
      "CV score for class identity_hate is 0.9652188230524432\n",
      "Total CV score is 0.9738837488324866\n"
     ]
    }
   ],
   "source": [
    "linear_regression_train_model('./input/gunControlComments.csv', 'gunCLR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "def LSTM_training(test_path, result):\n",
    "    test = pd.read_csv(test_path).fillna('NAN')\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "    list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    y = train[list_classes].values\n",
    "    list_sentences_test = test[\"Comment\"].fillna(\"CVxTz\").values\n",
    "\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    file_path=\"weights_base.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "    callbacks_list = [checkpoint, early] \n",
    "    model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    y_test = model.predict(X_te)\n",
    "    submid = pd.DataFrame({'id': test[\"Username\"]})\n",
    "    subcommet = pd.DataFrame({'comment': test[\"Comment\"]})\n",
    "    submission = pd.concat([submid, subcommet, pd.DataFrame(y_test, columns = list_classes)], axis=1)\n",
    "    submission.to_csv(result, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 448s 10ms/step - loss: 0.0885 - acc: 0.9740 - val_loss: 0.0595 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05953, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 438s 10ms/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0585 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05953 to 0.05851, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/politic/altRightSubredditBannedComments.csv', 'altRightLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "18000/18000 [==============================] - 196s 11ms/step - loss: 0.0440 - acc: 0.9829 - val_loss: 0.0614 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06141, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "18000/18000 [==============================] - 184s 10ms/step - loss: 0.0378 - acc: 0.9855 - val_loss: 0.0623 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/politic/abortionComments.csv', 'abbortionCommentsLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "18000/18000 [==============================] - 186s 10ms/step - loss: 0.1039 - acc: 0.9700 - val_loss: 0.0630 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06305, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "18000/18000 [==============================] - 180s 10ms/step - loss: 0.0551 - acc: 0.9800 - val_loss: 0.0597 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06305 to 0.05970, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/politic/donaldTrumpElectionWinComments.csv', 'DTwinLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "18000/18000 [==============================] - 187s 10ms/step - loss: 0.0372 - acc: 0.9857 - val_loss: 0.0629 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06291, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "18000/18000 [==============================] - 197s 11ms/step - loss: 0.0346 - acc: 0.9863 - val_loss: 0.0680 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/politic/gunControlComments.csv', 'gunControlLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "18000/18000 [==============================] - 194s 11ms/step - loss: 0.0348 - acc: 0.9865 - val_loss: 0.0733 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07331, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "18000/18000 [==============================] - 10028s 557ms/step - loss: 0.0313 - acc: 0.9878 - val_loss: 0.0679 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07331 to 0.06791, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/politic/marijuanaJustlikecigar.csv', 'marijuanaJustlikecigarLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "18000/18000 [==============================] - 170s 9ms/step - loss: 0.0278 - acc: 0.9891 - val_loss: 0.0701 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07006, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "18000/18000 [==============================] - 168s 9ms/step - loss: 0.0250 - acc: 0.9896 - val_loss: 0.0860 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/politic/ObamabetterthanAllRep.csv', 'ObamabetterthanAllRepLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 439s 10ms/step - loss: 0.0908 - acc: 0.9727 - val_loss: 0.0533 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05333, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 483s 11ms/step - loss: 0.0504 - acc: 0.9814 - val_loss: 0.0500 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05333 to 0.04997, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/sports/ItalyFailWC.csv', 'ItalyFailWCLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 453s 10ms/step - loss: 0.0422 - acc: 0.9841 - val_loss: 0.0519 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05194, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 467s 10ms/step - loss: 0.0363 - acc: 0.9858 - val_loss: 0.0579 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/sports/RomareverseBarca.csv', 'RomareverseBarcaLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 447s 10ms/step - loss: 0.0359 - acc: 0.9861 - val_loss: 0.0576 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05762, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 454s 10ms/step - loss: 0.0311 - acc: 0.9879 - val_loss: 0.0557 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05762 to 0.05570, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "LSTM_training('./input/sports/RonaldotoMonaco.csv', 'RonaldotoMonacoLSTM.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
